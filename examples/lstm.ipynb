{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this Jupyter Notebook, we will be discussing Long Short-Term Memory\n",
    "(LSTM) models implemented in PyTorch. LSTM is a type of recurrent neural\n",
    "network (RNN) that is widely used for modeling sequential data due to\n",
    "its ability to capture long-term dependencies in the data.\n",
    "\n",
    "We will start by introducing the basic architecture of an LSTM model and\n",
    "the LSTM cell, which is the basic building block of an LSTM. Then, we\n",
    "will discuss how to implement an LSTM model in PyTorch and train it on a\n",
    "dataset. Finally, we will evaluate the performance of the trained model\n",
    "and discuss some of the common challenges and tips for improving the\n",
    "performance of LSTM models.\n",
    "\n",
    "## LSTM Architecture\n",
    "\n",
    "The architecture of an LSTM model consists of multiple LSTM cells, each\n",
    "of which has three gates: the input gate, forget gate, and output gate.\n",
    "These gates are responsible for regulating the information flow into and\n",
    "out of the cell, as well as determining which information to forget or\n",
    "retain.\n",
    "\n",
    "The LSTM cell has three main components:\n",
    "\n",
    "-   **Cell state:** This is the “memory” of the cell and is passed along\n",
    "    from one cell to another in a sequence.\n",
    "\n",
    "-   **Input gate:** Controls what information is stored in the cell.\n",
    "\n",
    "-   **Output gate:** Controls what information is output from the cell.\n",
    "\n",
    "The input, forget, and output gates are controlled by sigmoid functions,\n",
    "which output values between 0 and 1. The sigmoid functions take the\n",
    "input values and the previous hidden state as inputs and output values\n",
    "that determine how much of the input values should be allowed to pass\n",
    "through each gate.\n",
    "\n",
    "## Implementing an LSTM Model in PyTorch\n",
    "\n",
    "Now that we understand the basic architecture of an LSTM model, let’s\n",
    "discuss how to implement it in PyTorch.\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "The first step in building an LSTM model is to prepare the data. This\n",
    "involves loading the dataset and splitting it into training, validation,\n",
    "and test sets. We will then convert the dataset into PyTorch tensors\n",
    "using the DataLoader class."
   ],
   "id": "19d317a7-7332-4e68-89fc-317a82a4f94d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LSTMDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "        \n",
    "# Load and split the dataset\n",
    "train_data, val_data, test_data = load_data()\n",
    "        \n",
    "# Convert the dataset into PyTorch tensors\n",
    "train_dataset = LSTMDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "val_dataset = LSTMDataset(val_data)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "test_dataset = LSTMDataset(test_data)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ],
   "id": "8652dd3b-2e35-48ab-9fb5-7ae8af6484fb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "Next, we will define the architecture of the LSTM model using the\n",
    "nn.LSTM class in PyTorch. We will also define a fully connected layer to\n",
    "output the prediction."
   ],
   "id": "895a0697-7960-44c2-a3a9-d049120105e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1])\n",
    "        return out"
   ],
   "id": "2230ece6-9606-49e0-aa05-49b2f29b6f2b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "\n",
    "Finally, we will train the LSTM model using the Adam optimizer and the\n",
    "Mean Squared Error (MSE) loss function. We will also evaluate the\n",
    "performance of the trained model on the validation and test sets."
   ],
   "id": "b6a5d709-e400-4153-a747-1b5a10c4ccdc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the hyperparameters\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "\n",
    "# Initialize the model and the optimizer\n",
    "lstm = LSTM(input_size, hidden_size, num_layers, dropout)\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=lr)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        x, y = data[:, :-1], data[:, -1]\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = lstm(x)\n",
    "        loss = nn.MSELoss()(y_pred.squeeze(), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_idx, data in enumerate(val_loader):\n",
    "            x, y = data[:, :-1], data[:, -1]\n",
    "            y_pred = lstm(x)\n",
    "            val_loss += nn.MSELoss()(y_pred.squeeze(), y)\n",
    "        val_loss /= len(val_loader)\n",
    "    \n",
    "    print('Epoch: {} \\t Training Loss: {:.6f} \\t Validation Loss: {:.6f}'.format(epoch+1, loss.item(), val_loss.item()))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "with torch.no_grad():\n",
    "    test_loss = 0\n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        x, y = data[:, :-1], data[:, -1]\n",
    "        y_pred = lstm(x)\n",
    "        test_loss += nn.MSELoss()(y_pred.squeeze(), y)\n",
    "    test_loss /= len(test_loader)\n",
    "    \n",
    "print('Test Loss: {:.6f}'.format(test_loss.item()))"
   ],
   "id": "1cada2c3-4aa6-4d89-80f6-e09f706853dc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this Jupyter Notebook, we discussed Long Short-Term Memory (LSTM)\n",
    "models implemented in PyTorch. We introduced the basic architecture of\n",
    "an LSTM model and the LSTM cell, which is the basic building block of an\n",
    "LSTM. We also discussed how to implement an LSTM model in PyTorch and\n",
    "train it on a dataset. Finally, we evaluated the performance of the\n",
    "trained model and discussed some common challenges and tips for\n",
    "improving the performance of LSTM models.\n",
    "\n",
    "## References\n",
    "\n",
    "1.  Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory.\n",
    "    Neural Computation, 9(8), 1735-1780.\n",
    "\n",
    "2.  PyTorch documentation. (n.d.). Retrieved from\n",
    "    https://pytorch.org/docs/stable/index.html."
   ],
   "id": "bf6a8b0a-70f8-4f46-8dc6-64f2c79f974d"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}

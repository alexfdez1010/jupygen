{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Long short-term memory (LSTM) is a type of neural network architecture\n",
    "designed to capture long-term dependencies in sequential data. It is\n",
    "particularly useful in time-series analysis, language modeling, speech\n",
    "recognition, and other tasks that require modeling sequential data.\n",
    "PyTorch is a popular open-source machine learning framework that\n",
    "provides excellent support for building and training LSTMs.\n",
    "\n",
    "In this notebook, we will explore the basics of LSTMs in PyTorch. We\n",
    "will start with the theoretical background, including the architecture\n",
    "of LSTMs and how they work. Then, we will dive into practical\n",
    "implementation, including data preprocessing, building the LSTM model,\n",
    "training the model, and evaluating the model’s performance. Finally, we\n",
    "will discuss some advanced techniques for LSTMs, such as bidirectional\n",
    "LSTMs, stacked LSTMs, and attention mechanisms.\n",
    "\n",
    "# Theoretical Background\n",
    "\n",
    "## Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Before we dive into LSTMs, let’s first take a look at recurrent neural\n",
    "networks (RNNs). RNNs are a type of neural network architecture that is\n",
    "designed to handle sequential data. They process sequences of vectors\n",
    "(inputs) by recursively applying the same set of weights to each input\n",
    "vector. An RNN is composed of a loop that processes each input vector\n",
    "and updates its internal state based on the input vector and the\n",
    "previous state. The output of the RNN at each step is a function of the\n",
    "current internal state.\n",
    "\n",
    "The standard RNN architecture suffers from a fundamental problem called\n",
    "the vanishing gradient problem. This problem arises when the gradient\n",
    "that is propagated through time becomes very small, making it difficult\n",
    "for the model to learn long-term dependencies. LSTMs were introduced to\n",
    "address this problem.\n",
    "\n",
    "## LSTMs\n",
    "\n",
    "LSTMs were first proposed by Hochreiter and Schmidhuber in 1997 as a\n",
    "variant of RNNs. They are designed to capture long-term dependencies by\n",
    "introducing a memory cell and three gating mechanisms: the input gate,\n",
    "the output gate, and the forget gate.\n",
    "\n",
    "The memory cell is the core component of LSTMs. It can remember\n",
    "information for an extended period and selectively choose which\n",
    "information to forget or remember using the gating mechanisms. The input\n",
    "gate controls the amount of new information that enters the memory cell.\n",
    "The forget gate controls the amount of old information that is discarded\n",
    "from the memory cell. The output gate controls the amount of information\n",
    "that is outputted from the memory cell.\n",
    "\n",
    "The LSTM architecture is illustrated in the figure below.\n",
    "\n",
    "<figure>\n",
    "<img\n",
    "src=\"https://raw.githubusercontent.com/abulbasar/data-science-notebooks/master/images/lstm.png\"\n",
    "alt=\"LSTM Architecture\" />\n",
    "<figcaption aria-hidden=\"true\">LSTM Architecture</figcaption>\n",
    "</figure>\n",
    "\n",
    "The equations that govern the state updates in an LSTM are given below:\n",
    "\n",
    "$$\n",
    "i_t = \\sigma(W_{i} x_t + U_{i} h_{t-1} + b_i) \\\\\n",
    "f_t = \\sigma(W_{f} x_t + U_{f} h_{t-1} + b_f) \\\\\n",
    "o_t = \\sigma(W_{o} x_t + U_{o} h_{t-1} + b_o) \\\\\n",
    "\\tilde{c}_t = \\text{tanh}(W_{c} x_t + U_{c} h_{t-1} + b_c) \\\\\n",
    "c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t \\\\\n",
    "h_t = o_t \\odot \\text{tanh}(c_t)\n",
    "$$\n",
    "\n",
    "where $x_t$ is the input at time step $t$, $h_t$ is the hidden state at\n",
    "time step $t$, $c_t$ is the memory cell at time step $t$, $i_t$, $f_t$,\n",
    "and $o_t$ are the input, forget, and output gates, respectively. $W$ and\n",
    "$U$ are the learnable weight matrices, and $b$ is the bias vector.\n",
    "$\\sigma$ is the sigmoid activation function, and $\\odot$ is the\n",
    "element-wise multiplication.\n",
    "\n",
    "# Practical Implementation\n",
    "\n",
    "Now that we have a good understanding of the theory behind LSTMs let’s\n",
    "dive into the practical implementation of an LSTM model in PyTorch. We\n",
    "will use the famous MNIST dataset to classify handwritten digits.\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "We will start by loading the MNIST dataset and preprocessing it. This\n",
    "dataset contains images of handwritten digits from 0 to 9.\n",
    "\n",
    "``` python\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load training and testing datasets\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "```\n",
    "\n",
    "We use `transforms.ToTensor()` to convert the images to PyTorch tensors\n",
    "and then normalize the pixel values using `transforms.Normalize()`. We\n",
    "then load the training and testing datasets and create data loaders to\n",
    "iterate over the data during training and testing.\n",
    "\n",
    "## Building the LSTM Model\n",
    "\n",
    "Next, we will define the LSTM model architecture using PyTorch’s\n",
    "`nn.LSTM()` module. We will use one LSTM layer with 128 hidden units,\n",
    "followed by a fully connected layer with 10 outputs for the 10 classes.\n",
    "\n",
    "``` python\n",
    "import torch.nn as nn\n",
    "\n",
    "class MNISTLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MNISTLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward pass through LSTM layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Only use the last output of the sequence\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Forward pass through the fully connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Create the model\n",
    "input_size = 28 # The size of each image is 28x28 pixels\n",
    "hidden_size = 128 # Number of hidden units in the LSTM layer\n",
    "num_classes = 10 # There are 10 classes (digits 0 through 9)\n",
    "model = MNISTLSTM(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Print the model's architecture\n",
    "print(model)\n",
    "```\n",
    "\n",
    "We define the `MNISTLSTM` class that inherits from PyTorch’s\n",
    "`nn.Module`. In the constructor, we define an LSTM layer with 128 hidden\n",
    "units and a fully connected layer with 10 outputs for the 10 classes.\n",
    "During forward pass, we first initialize the hidden state and cell state\n",
    "of the LSTM layer to zeros. We then pass the input sequence through the\n",
    "LSTM layer and use only the last output of the sequence. Finally, we\n",
    "pass the last output through the fully connected layer to get the output\n",
    "logits.\n",
    "\n",
    "## Training the Model\n",
    "\n",
    "Now that we have defined the LSTM model, let’s train it on the MNIST\n",
    "dataset. We will use the cross-entropy loss and the Adam optimizer for\n",
    "training. We will train the model for 5 epochs, which means that we will\n",
    "iterate over the training dataset 5 times.\n",
    "\n",
    "``` python\n",
    "epochs = 5\n",
    "lr = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "\n",
    "        # Flatten the images and convert them to sequences\n",
    "        images = images.view(images.shape[0], 28, 28)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass, backward pass, and optimization\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print('Epoch {}/{} Loss: {}'.format(epoch+1, epochs, running_loss/len(trainloader)))\n",
    "```\n",
    "\n",
    "In each epoch, we iterate over the training dataset and perform a\n",
    "forward pass, backward pass, and optimization. We flatten the images and\n",
    "convert them to sequences with 28 time steps (one for each row of the\n",
    "image). We then compute the cross-entropy loss between the predicted\n",
    "logits and the true labels and update the model’s weights using the Adam\n",
    "optimizer.\n",
    "\n",
    "## Evaluating the Model\n",
    "\n",
    "Now that we have trained the model, let’s evaluate its performance on\n",
    "the testing dataset. We will compute the accuracy, precision, recall,\n",
    "and F1 score of the model.\n",
    "\n",
    "``` python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "\n",
    "        # Flatten the images and convert them to sequences\n",
    "        images = images.view(images.shape[0], 28, 28)\n",
    "\n",
    "        # Predict the class labels\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Collect the true and predicted labels for evaluation\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(predicted.numpy())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, average='macro')\n",
    "rec = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print('Accuracy: {:.2f}%  Precision: {:.2f}  Recall: {:.2f}  F1 Score: {:.2f}'.format(\n",
    "    acc * 100, prec, rec, f1))\n",
    "```\n",
    "\n",
    "We iterate over the testing dataset and perform inference on each image\n",
    "by passing it through the trained model. We compute the accuracy,\n",
    "precision, recall, and F1 score of the model using scikit-learn’s\n",
    "metrics functions.\n",
    "\n",
    "# Advanced Techniques\n",
    "\n",
    "In this notebook, we have covered the basics of LSTMs in PyTorch,\n",
    "including the architecture, theory, implementation, training, and\n",
    "evaluation. LSTMs are a powerful type of neural network architecture\n",
    "that can learn long-term dependencies in sequential data. However, there\n",
    "are many advanced techniques that can be used to improve their\n",
    "performance, such as bidirectional LSTMs, stacked LSTMs, and attention\n",
    "mechanisms.\n",
    "\n",
    "## Bidirectional LSTMs\n",
    "\n",
    "Bidirectional LSTMs (BiLSTMs) are a variant of LSTMs that process\n",
    "sequential data in both forward and backward directions. They introduce\n",
    "another set of hidden states that process the sequence in reverse order.\n",
    "The final output is a concatenation of the forward and backward hidden\n",
    "states. BiLSTMs are useful when the model needs to predict the next\n",
    "timestep in the sequence based on both past and future timesteps.\n",
    "\n",
    "## Stacked LSTMs\n",
    "\n",
    "Stacked LSTMs are a variant of LSTMs that stack multiple LSTMs on top of\n",
    "each other. Each LSTM layer in the stack processes the output of the\n",
    "previous layer. Stacked LSTMs can capture more complex features and\n",
    "long-term dependencies in sequential data.\n",
    "\n",
    "## Attention Mechanisms\n",
    "\n",
    "Attention mechanisms are a type of neural network architecture that\n",
    "learns to selectively focus on parts of the input sequence when making\n",
    "predictions. In the context of LSTMs, attention mechanisms can be used\n",
    "to assign different weights to the time steps in the input sequence\n",
    "based on their importance. This can improve the accuracy and\n",
    "interpretability of the model.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this notebook, we have covered the basics of LSTMs in PyTorch. We\n",
    "started with the theoretical background, including the architecture of\n",
    "LSTMs and how they work. We then dove into practical implementation,\n",
    "including data preprocessing, building the LSTM model, training the\n",
    "model, and evaluating the model’s performance. Finally, we discussed\n",
    "some advanced techniques for LSTMs, such as bidirectional LSTMs, stacked\n",
    "LSTMs, and attention mechanisms."
   ],
   "id": "77f3f68f-cd1d-45d4-ae7f-f2b74bce0a59"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}

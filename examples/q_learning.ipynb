{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning in Python\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Deep Q-Learning is a type of reinforcement learning technique that uses\n",
    "deep neural networks to approximate the optimal Q-function. Q-function\n",
    "is the expected total future reward given a state and an action. Deep\n",
    "Q-Networks (DQNs) are used to learn the optimal Q-values of all possible\n",
    "actions for each state in a given environment.\n",
    "\n",
    "In this Jupyter Notebook, we will implement a basic DQN algorithm in\n",
    "Python using the popular deep learning library, Keras. We will train our\n",
    "DQN on the classic Atari game, Breakout.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before you start working on this notebook, make sure you have the\n",
    "following libraries installed:\n",
    "\n",
    "-   `tensorflow`\n",
    "-   `keras`\n",
    "-   `numpy`\n",
    "-   `gym`\n",
    "\n",
    "You can install the libraries by running the following command:"
   ],
   "id": "59fcda9f-ba2a-4f23-889f-bc92600b166b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow keras numpy gym"
   ],
   "id": "a5e1214a-5e5f-42cb-a9a9-cca0cd2e20e1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Environment\n",
    "\n",
    "We will use the `gym` library to create and interact with the Breakout\n",
    "environment. The Breakout environment is a 2D video game in which the\n",
    "player moves a paddle to hit a ball and break bricks. The goal of the\n",
    "game is to break as many bricks as possible without letting the ball\n",
    "fall off the screen.\n",
    "\n",
    "The environment has the following attributes:\n",
    "\n",
    "-   `observation_space`: a `Box` object representing the observation\n",
    "    space (a 210x160 RGB image).\n",
    "-   `action_space`: a `Discrete` object representing the action space\n",
    "    (move left, move right, do nothing).\n",
    "-   `reward_range`: a tuple representing the minimum and maximum reward\n",
    "    achievable (0 and 1, respectively).\n",
    "\n",
    "We can create the environment as follows:"
   ],
   "id": "c7b50f60-63a7-4904-bf1e-9f1f2caffa03"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('Breakout-v0')"
   ],
   "id": "3c6cf506-cea5-4fc9-9095-3772cf9b9e9a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the DQN Algorithm\n",
    "\n",
    "### Step 1: Initialize the DQN model\n",
    "\n",
    "We will create a deep neural network with three convolutional layers and\n",
    "two fully connected layers. The input to the network will be a 84x84\n",
    "grayscale image, which is a preprocessed version of the original\n",
    "observation. The output of the network will be a Q-value for each\n",
    "possible action."
   ],
   "id": "06377ef6-0542-431e-ac86-810edc9c5080"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "\n",
    "def create_model(input_shape, num_actions):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, 8, strides=(4, 4), activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(64, 4, strides=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(64, 3, strides=(1, 1), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(num_actions, activation='linear'))\n",
    "    return model\n",
    "\n",
    "input_shape = (84, 84, 4)\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "model = create_model(input_shape, num_actions)"
   ],
   "id": "58dc0448-5930-4a48-9504-0479e42542c8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the target network\n",
    "\n",
    "The target network is a copy of the DQN model that is used to compute\n",
    "the target Q-values during training. We will update the target network\n",
    "every `tau` steps."
   ],
   "id": "d7a57138-a488-4d01-b471-eaea7909d008"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_model(model):\n",
    "    target_model = create_model(model.input_shape[1:], model.output_shape[-1])\n",
    "    target_model.set_weights(model.get_weights())\n",
    "    return target_model\n",
    "\n",
    "target_model = create_target_model(model)\n",
    "\n",
    "tau = 10000"
   ],
   "id": "644c0095-efe5-4887-ac0e-29345ab87f1f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the replay buffer\n",
    "\n",
    "The replay buffer is a data structure that stores the agentâ€™s\n",
    "experiences (state, action, reward, next state) during training. We will\n",
    "use a deque to store the experiences, and we will sample batches of\n",
    "experiences from the replay buffer to train the DQN model."
   ],
   "id": "e9a5f19e-fe3d-42d4-9962-4f25323f305c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        idxs = np.random.choice(len(self.buffer), size=batch_size, replace=False)\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in idxs:\n",
    "            state, action, reward, next_state, done = self.buffer[i]\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "\n",
    "max_size = 1000000\n",
    "\n",
    "buffer = ReplayBuffer(max_size)"
   ],
   "id": "b7d889a6-96f7-4f82-af36-571bc23af655"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define the epsilon-greedy policy\n",
    "\n",
    "The epsilon-greedy policy is used to balance exploration and\n",
    "exploitation during training. We will start with a high exploration rate\n",
    "(`epsilon`) and gradually decrease it over time."
   ],
   "id": "bd449467-5923-455f-8e35-6b470d6f0482"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(num_actions)\n",
    "    else:\n",
    "        Q_values = model.predict(state)\n",
    "        return np.argmax(Q_values[0])\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay = 1000000\n",
    "\n",
    "def get_epsilon(step):\n",
    "    return max(epsilon_end, epsilon_start - (epsilon_start - epsilon_end) * step / epsilon_decay)"
   ],
   "id": "0efc7c04-07d7-47c7-a838-18289ac81ab9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Implement the training loop\n",
    "\n",
    "In the training loop, we will do the following:\n",
    "\n",
    "-   Reset the environment and preprocess the initial observation.\n",
    "-   Repeat the following until the episode is done:\n",
    "    -   Choose an action using the epsilon-greedy policy.\n",
    "    -   Execute the action and observe the next state, reward, and\n",
    "        whether the episode is done.\n",
    "    -   Store the experience in the replay buffer.\n",
    "    -   Sample a batch of experiences from the replay buffer.\n",
    "    -   Compute the target Q-values using the target network.\n",
    "    -   Compute the predicted Q-values using the DQN model.\n",
    "    -   Compute the loss between the target Q-values and the predicted\n",
    "        Q-values.\n",
    "    -   Backpropagate the loss and update the DQN model.\n",
    "    -   Every `tau` steps, update the target network weights with the\n",
    "        DQN weights."
   ],
   "id": "a566909e-cf97-42c9-b473-8595634d6233"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "num_episodes = 10000\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "for i in trange(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = preprocess_state(state)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        epsilon = get_epsilon(i)\n",
    "        action = epsilon_greedy_policy(state, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = preprocess_state(next_state)\n",
    "        buffer.add((state, action, reward, next_state, done))\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if len(buffer.buffer) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "            targets = rewards + (1 - dones) * gamma * np.amax(target_model.predict(next_states), axis=1, keepdims=True)\n",
    "            Q_values = model.predict(states)\n",
    "            Q_values[range(batch_size), actions] = targets.flatten()\n",
    "            model.fit(states, Q_values, verbose=0)\n",
    "            \n",
    "            if i % tau == 0:\n",
    "                target_model.set_weights(model.get_weights())\n",
    "                \n",
    "    episode_rewards.append(episode_reward)\n",
    "    print('Episode {}/{}: reward={}'.format(i+1, num_episodes, episode_reward))"
   ],
   "id": "3c7b8007-da4c-48bd-a9cb-5169a48efe5c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Evaluate the performance\n",
    "\n",
    "We can evaluate the performance of the trained DQN by running the\n",
    "following code:"
   ],
   "id": "d665e451-57f2-4805-a1c6-2fb4639405ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval_episodes = 10\n",
    "\n",
    "eval_rewards = []\n",
    "\n",
    "for i in range(num_eval_episodes):\n",
    "    state = env.reset()\n",
    "    state = preprocess_state(state)\n",
    "    done = False\n",
    "    eval_reward = 0\n",
    "    while not done:\n",
    "        Q_values = model.predict(state)\n",
    "        action = np.argmax(Q_values[0])\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = preprocess_state(next_state)\n",
    "        eval_reward += reward\n",
    "        state = next_state\n",
    "    eval_rewards.append(eval_reward)\n",
    "    print('Evaluation Episode {}/{}: reward={}'.format(i+1, num_eval_episodes, eval_reward))\n",
    "    \n",
    "print('Average reward =', np.mean(eval_rewards))"
   ],
   "id": "177dce4a-b5f8-4156-81f0-c2f4ad0316c2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have implemented a basic DQN algorithm in Python\n",
    "using Keras. We have trained our DQN on the Atari game, Breakout, and\n",
    "evaluated its performance. We hope this notebook has helped you\n",
    "understand how DQN works and how to implement it in Python."
   ],
   "id": "3c05de6d-22c9-423a-8482-75b434848d22"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
